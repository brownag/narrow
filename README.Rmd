---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
library(arrow)
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# carrow

<!-- badges: start -->
[![Codecov test coverage](https://codecov.io/gh/paleolimbot/carrow/branch/master/graph/badge.svg)](https://codecov.io/gh/paleolimbot/carrow?branch=master)
[![R-CMD-check](https://github.com/paleolimbot/carrow/workflows/R-CMD-check/badge.svg)](https://github.com/paleolimbot/carrow/actions)
[![Lifecycle: experimental](https://img.shields.io/badge/lifecycle-experimental-orange.svg)](https://lifecycle.r-lib.org/articles/stages.html#experimental)
<!-- badges: end -->

The goal of carrow is to wrap the [Arrow Data C API](https://arrow.apache.org/docs/format/CDataInterface.html) and [Arrow Stream C API](https://arrow.apache.org/docs/format/CStreamInterface.html) to provide lightweight Arrow support for R packages to consume and produce streams of data in Arrow format.

## Installation

You can install the development version from [GitHub](https://github.com/) with:

``` r
# install.packages("remotes")
remotes::install_github("paleolimbot/carrow")
```

## Creating arrays

You can create an Arrow array using `as_carrow_array()`. For many types (e.g., integers and doubles), this is done without any copying of memory: carrow just arranges the existing R vector memory and protects it for the lifetime of the underlying `struct ArrowArray`.

```{r example}
library(carrow)
(array <- as_carrow_array(1:5))
```

For `Array`s and `RecordBatch`es from the [arrow package](https://arrow.apache.org/docs/r/), this is almost always a zero-copy operation and is instantaneous even for very large Arrays.

```{r}
library(arrow)
(array2 <- as_carrow_array(Array$create(1:5)))
```

## Exporting arrays

To convert an array object to some other type, use `from_carrow_array()`:

```{r}
str(from_carrow_array(array))
```

The carrow package has built-in defaults for converting arrays to R objects; you can also specify your own using the `ptype` argument:

```{r}
str(from_carrow_array(array, ptype = double()))
from_carrow_array(array, ptype = arrow::Array)
```

## Streams

The Arrow C API also specifies an experimental stream interface. In addition to handling streams created elsewhere, you can create streams based on a `carrow_array()`:

```{r}
stream1 <- as_carrow_array_stream(as_carrow_array(1:3))
carrow_array_stream_get_next(stream1)
carrow_array_stream_get_next(stream1)
```

...or based on a function that returns one or more `carrow_array()`s:

```{r}
counter <- -1
rows_per_chunk <- 5
csv_file <- readr::readr_example("mtcars.csv")
schema <- as_carrow_array(
  readr::read_csv(
    csv_file, 
    n_max = 0,
    col_types = readr::cols(.default = readr::col_double())
  )
)$schema

stream2 <- carrow_array_stream_function(schema, function() {
  counter <<- counter + 1L
  result <- readr::read_csv(
    csv_file, 
    skip = 1 + (counter * rows_per_chunk),
    n_max = rows_per_chunk,
    col_names = c(
      "mpg", "cyl", "disp", "hp", "drat",
      "wt", "qsec", "vs", "am", "gear", "carb"
    ),
    col_types = readr::cols(.default = readr::col_double())
  )
  
  if (nrow(result) > 0) result else NULL
})
```

You can pass these to Arrow as a `RecordBatchReader` using `carrow_array_stream_to_arrow()`:

```{r}
reader <- carrow_array_stream_to_arrow(stream2)
as.data.frame(reader$read_table())
```

Currently attemping to export an arrow `RecordBatchReader()` segfaults for an unknown reason, but in theory one could also go the other direction.
