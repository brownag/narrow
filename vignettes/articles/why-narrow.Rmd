---
title: "Why 'narrow'?"
---

```{r, include = FALSE}
library(cpp11)
library(narrow)
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

The [Apache Arrow](https://arrow.apache.org/) project is awesome. So is the [R package](https://arrow.apache.org/docs/r/) that provides bindings to the C++ API in a way that mirrors how it's done in [Python](https://arrow.apache.org/docs/python/) and adds a [dplyr](https://dplyr.tidyverse.org/) interface to the single-node query engine. The 'arrow' package for R provides an end-to-end solution for querying and computing on in-memory and bigger-than-memory data sets, including a particularly slick workflow for [hosting data on cloud storage and querying it locally](https://arrow.apache.org/docs/r/articles/fs.html). It's a fantastic user-facing package that makes it possible for users familiar with [dplyr](https://dplyr.tidyverse.org/) to work with larger-than-memory (and larger-than-disk!) data without learning a new skillset.

Apache Arrow also has a lot to offer the R package ecosystem as a developer-facing package. R doesn't natively support many useful types (e.g., fixed-precision decimals and 64-bit integers) and starts to get slow when dealing with really big (millions of elements) character vectors and nested lists. Arrow supports a rich set of types and can slice and rearrange tens of millions of strings without stuttering. Zero-copy transfer between R, Python, Rust, and other language bindings make it possible to take advantage of processing functions exposed in other languages without writing to disk. The parquet, feather, and IPC formats are open, cross-platform versions of Arrow types that are fast to read and write (compared to CSV or serializing R objects) and provide the same rich set of types as in-memory Arrow arrays.

I'm going to make the argument here that while the arrow R package is an fantastic user-facing package, it's a difficult package for R developers to use as a dependency.

Stepping back from Arrow, it's worth taking a look at the [DBI package](https://dbi.r-dbi.org/), which is several decades old, has 181 reverse dependencies (972 if you count recursive dependencies), and is *the* package to implement if you have a database engine and you want R users to use it. It defines three object types (`DBIDriver`, `DBIConnection`, and `DBIResult`) and the methods needed to connect to a database, make a query, get and R object back.

- `dbConnect(<DBIDriver>)` -> `DBIConnection`
- `dbSendQuery(<DBIConnection>)` -> `DBIResult`
- `dbFetch(res)` -> `data.frame`

The sources are about 700 KB and installed the package takes up about 1.2 MB. This is actually a medium-sized R package in terms of size and lines of code (about 1800), but has zero dependencies and installs in a few seconds. The main thing is that nobody has ever noticed the DBI package being installed nor had any install problems with it. That's makes it really easy for a package developer to add as a dependency.

```{r, eval=FALSE, echo=FALSE}
length(tools::dependsOnPkgs("DBI", recursive = FALSE, installed = available.packages()))

curl::curl_download(
  "https://github.com/r-dbi/DBI/archive/refs/heads/main.zip",
  "DBI.zip"
)

unzip("DBI.zip", exdir = ".")

system("cloc DBI.zip")
sum(file.size(list.files(system.file("", package = "DBI"), recursive = TRUE, full.names = TRUE))) / (2 ^ 20)
```

As a user, if you've heard about the arrow R package and want to use it, it's easy to install:

```r
install.packages("arrow")
```

...unless you're on Linux, in which case it's still pretty easy but you have to do this:

```r
Sys.setenv(NOT_CRAN = "true")
install.packages("arrow")
```

Both of those will install a pre-compiled version of the underlying C++ library. If you don't, it will take a 10 minutes or so to build. Even with the pre-compiled underlying C++ library, it takes about 60 seconds to install the package. There are a few tricks you can use to speed this up that are detailed in a [truly excellent install guide](https://arrow.apache.org/docs/r/articles/install.html), which, as a user excited about Arrow who just typed `install.packages("arrow")`, is an excellent resource.

Pretend for a moment, though, that you just typed this:

```r
install.packages("somePackageDependingOnArrow")
```

Most R users on Linux (or using Linux via Docker or rhub) will spend about 15 minutes waiting for arrow to install. As a user excited about the somePackageDependingOnArrow package that might have no clue what Arrow is or why 'arrow' is taking so long to install, this is annoying at best. If a package in turn depends on somePackageDependingOnArrow, there's even less reason for the user to care about looking up the install instructions. The arrow package *is* really useful and many of the things it does are impossible to do otherwise, so in many cases dealing with that install friction is worth it as the maintainer of somePackageDependingOnArrow. For many other maintainers, it's probably easier to just not use Arrow.

One way to ease this friction is to make the troublesome `Sys.setenv(NOT_CRAN = "true")` line go away. It's necessary because CRAN doesn't allow packages to download binaries by default (with a very large asterisk, because many packages do this anyway). The policy is there in the name of transparency to make sure that binaries served by CRAN can be inspected for malicious code and/or inspected for code quality. It is absolutely possible to download a binary from a trusted source in a transparent way, and this should be made possible and easy to do by CRAN. That battle can and should be fought but will take some time (my guess is a few years).

Even if there is a binary available for Arrow C++ and it *was* easy to download and link to it in a CRAN package, the scope of the 'arrow' package is such that it's still going to take over a minute to compile. A minute is very good for a package that can query huge amounts of data stored on the cloud using familiar dplyr syntax. A minute is very bad for a package that just wants to read and write something better than a CSV. Again, there are tricks to speed this up, but getting users to read an install guide for a first, second, or third order dependency is always going to be a source of friction.

## Scope

I think the main goal of a developer-facing Apache Arrow R package should be to make it easy to use, easy to test, and safe to pass around external pointers to C API structures. These structures are ABI stable and are safe to pass around among R packages and embedded interpreters. If a dependency package wants to operate on an array of unsigned 64-bit integers (like an [S2 cell identifier](https://r-spatial.github.io/s2/reference/s2_cell.html#details)), all they should have to do is run the input through `as_narrow_array()`. The package doing the operating doesn't have to know about how or why the input came to be a pointer to a `struct ArrowArray` and a `struct ArrowSchema`. Similarly, any package that wants to represent an object as an array of unsigned 64-bit integers just has to create the appropriate `struct ArrowArray` and `struct ArrowSchema`.

First, I'll demonstrate "easy". For this example, we need some C or C++ code:

```{cpp11}
#include <cpp11.hpp>
#include <narrow.h>

using namespace cpp11;

[[cpp11::linking_to(narrow)]]
[[cpp11::register]]
void print_uint64_cpp(SEXP array_data_xptr) {
  struct ArrowArray* array_data = safe[array_data_from_xptr](array_data_xptr, "array_data");
  const uint64_t* data = reinterpret_cast<const uint64_t*>(array_data->buffers[1]);
  for (int64_t i = 0; i < array_data->length; i++) {
    Rprintf("%llu\n", data[i]);
  }
}
```

And an R wrapper:

```{r}
print_uint64 <- function(x) {
  x <- as_narrow_array(x, schema = narrow_schema("L"))
  stopifnot(identical(x$schema$format, "L"))
  print_uint64_cpp(x$array_data)
}
```

Because I've implemented `as_narrow_array()` for arrow `Array` objects, they can be used as input without knowing that the arrow package exists.

```{r}
print_uint64(arrow::Array$create(1:5)$cast(arrow::uint64()))
```

This example doesn't require extra code to make it safe, either, because `array_data_from_xptr()` (defined in `narrow.h`) checks that the `struct ArrowArray` is not `NULL` nor has a `NULL` release callback. Keen onlookers will notice that it doesn't check the validity buffer for null elements, nor does it check the number of buffers, both of which I'll come back to. The point is that iterating over Arrow arrays takes 6 lines of C++ and 5 lines of R. There's no configure script, and the narrow package prototype as I've implemented it here installs in a few seconds and has no dependencies (R dependencies nor system dependencies).

Next I'll demonstrate "easy to test". This particular array only has a few buffers and is reasonable to construct by hand given some knowledge of the format spec.

```{r}
library(testthat)

uint_raw_test <- function(x) {
  lapply(x, function(e) as.raw(c(e, rep(0x00, 7))))
}

uint_array <- narrow_array(
  narrow_schema("L"),
  narrow_array_data(
    list(NULL, unlist(uint_raw_test(1:3))),
    length = 5,
    null_count = 0
  )
)

expect_output(print_uint64(uint_array), "1\n2\n3\n")
```

It's reasonable to use the Arrow R package to generate test data too, since a development dependency doesn't get installed by default with `install.packages()`. If all you need are three unsigned 64-bit integers, though, I'd argue it's conceptually overkill to require contributors and CI systems to install arrow for R. It's also hard or impossible to create invalid arrays in R/arrow, which is useful to test validation code.

```{r}
uint_array$schema$format <- "l"
expect_error(
  print_uint64(uint_array),
  "identical.*?is not TRUE"
)

uint_array_invalid <- narrow_array(
  narrow_schema("L"),
  narrow:::narrow_allocate_array_data(),
  validate = FALSE
)

expect_error(
  print_uint64(uint_array_invalid),
  "has already been released"
)
```

The "safe" bit is related to "easy" because writing validation code can be tedious and hard to get right. The ability to write a data processing function that takes an Arrow array in 6 lines of C or C++ is closely related to the ability to pass off the responsibility for creating and validating those arrays elsewhere. Because of this, narrow makes it hard to generate obviously bad objects:

```{r, error=TRUE}
narrow_array(narrow_schema("L"), narrow_array_data(list()))
```

With the ability to create and validate there is a lot of extra scope...narrow has to know how many buffers are expected for all the types and it *should* know how long those buffers need to be (although this isn't implemented).

## Streams?

I haven't mentioned streams to keep this concise, but they're implemented in the prototype as well, including ways to create and consume them. Like arrays, both the creating and the consuming is essential for testing.

```{r}
stream <- narrow_array_stream(list(1:2, 3:5))
narrow_array_stream_collect(stream)
```

## Read, write, slice, filter, take, convert

These are extra things that I haven't implemented that I think should also be included.

- Read and write, because feather/IPC is so much better than CSV and everybody should be using it
- Slice, filter, and take, because it's hard to implement an extension vector type otherwise
- Convert (to and from R objects), because it makes testing a lot easier and makes it possible for more users to take advantage of Arrow types.

For all of these, it's easy to fall back to the arrow R package if it's installed or to fall back to the arrow R package for the hard bits (e.g., casting). I still think all three are worth implementing for a developer-facing R package. There's no need for any of this to be fast because if speed is an issue, users can be directed to the arrow R package.

## Too much scope?

I mentioned earlier that the DBI package is actually medium-sized R package (1 MB installed, which is mostly R code). While the general idea of DBI is simple, the reality of providing a common interface to many database backends is complex, so there are functions like `dbQuoteLiteral()` that are included that make functions like `glue::glue_sql()` (which generates SQL in a way that protects against injection attacks) possible. That functionality could have been outsourced to another package but then a package that wants to implement a database backend would have to import *two* packages. The extra functionality doesn't cost anything to install, so I'd argue it's worth it. Similarly, I'd argue that including the facilities to validate, read, write, and rearrange Arrow arrays is worth it, as long as it can be done without installation cost.

## A 'narrow' C library?

Perhaps the most obvious way to do this kind of thing would be to vendor in a minimal copy of Arrow C++ library and use it to implement the bits that I've mentioned above. Another way to do it is to write and use a tiny vendorable C library. I did it using the C library approach in the prototype because I was having a good time with it (I based the style off the excellent [tensorflow C API](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/c_api.h) and the structure off the [rlang C library](https://github.com/r-lib/rlang/tree/main/src/rlang) and [sqlite3](https://www.sqlite.org/download.html)), but that by no means requires that it be the direction a future 'narrow' R package needs to go.

I think the tiny vendorable C library is the right approach beccause it's not trivial to vendor C++ into an R package. Even if *we* can do it, it would be harder for a dependency package to take the same approach if they wanted to, for example, extract metadata from the schema, read the bitpacked validity buffer, or validate arrays themselves in compiled code. I'll also point to the success of SQLite, which ships as two files (sqlite3.h and sqlite3.c), is ubiquitous as a way to read and write typed data to disk, and is the basis for many file formats (e.g., the GeoPackage format for geospatial data).
